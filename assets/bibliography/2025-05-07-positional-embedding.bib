@inproceedings{46201, title={Attention is All You Need}, author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin}, year={2017}, URL={https://arxiv.org/pdf/1706.03762.pdf}}

@inproceedings{50650, title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, author={Alexander Kolesnikov and Alexey Dosovitskiy and Dirk Weissenborn and Georg Heigold and Jakob Uszkoreit and Lucas Beyer and Matthias Minderer and Mostafa Dehghani and Neil Houlsby and Sylvain Gelly and Thomas Unterthiner and Xiaohua Zhai}, year={2021}}

@inproceedings{46989, title={Self-Attention with Relative Position Representations}, author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani}, year={2018}}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}